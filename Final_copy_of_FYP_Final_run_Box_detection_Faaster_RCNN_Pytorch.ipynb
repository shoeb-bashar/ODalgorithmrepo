{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ub2Z7rO3NTeh"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJlviqV7UKBK"
      },
      "outputs": [],
      "source": [
        "!python -m pip install pyyaml==5.1\n",
        "import sys, os, distutils.core\n",
        "# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities (e.g. compiled operators).\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n",
        "!git clone 'https://github.com/facebookresearch/detectron2'\n",
        "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
        "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
        "sys.path.insert(0, os.path.abspath('./detectron2'))\n",
        "# Properly install detectron2. (Please do not install twice in both ways)\n",
        "# !python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z39BDkKBVgbR"
      },
      "outputs": [],
      "source": [
        "import torch, detectron2\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "print(\"detectron2:\", detectron2.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvZOwZL6VllG"
      },
      "outputs": [],
      "source": [
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUt9V5iOfNqL"
      },
      "outputs": [],
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"YeynZkK8Y71dBWtp7oX7\")\n",
        "project = rf.workspace(\"shoeb-bashar-4m364\").project(\"palletised_box\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"coco\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "EDnkJPy0Ju9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uos8SvIpfSE9"
      },
      "outputs": [],
      "source": [
        "# if your dataset is in COCO format, this cell can be replaced by the following three lines:\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "register_coco_instances(\"my_dataset_train\", {}, \"/content/palletised_box-1/train/_annotations.coco.json\", \"/content/palletised_box-1/train\")\n",
        "register_coco_instances(\"my_dataset_val\", {}, \"/content/palletised_box-1/valid/_annotations.coco.json\", \"/content/palletised_box-1/valid\")\n",
        "register_coco_instances(\"my_dataset_test\", {}, \"/content/palletised_box-1/test/_annotations.coco.json\", \"/content/palletised_box-1/test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TkvxyMvZn7O"
      },
      "outputs": [],
      "source": [
        "my_dataset_train_metadata = MetadataCatalog.get(\"my_dataset_train\")\n",
        "dataset_dicts = DatasetCatalog.get(\"my_dataset_train\")\n",
        "\n",
        "import random\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "\n",
        "for d in random.sample(dataset_dicts, 3):\n",
        "    img = cv2.imread(d[\"file_name\"])\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=my_dataset_train_metadata, scale=0.5)\n",
        "    out = visualizer.draw_dataset_dict(d)\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvyYC07zj_qz"
      },
      "outputs": [],
      "source": [
        "#We are importing our own Trainer Module here to use the COCO validation evaluation during training. Otherwise no validation eval occurs.\n",
        "\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.evaluation import COCOEvaluator\n",
        "\n",
        "class CocoTrainer(DefaultTrainer):\n",
        "\n",
        "  @classmethod\n",
        "  def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
        "\n",
        "    if output_folder is None:\n",
        "        os.makedirs(\"coco_eval\", exist_ok=True)\n",
        "        output_folder = \"coco_eval\"\n",
        "\n",
        "    return COCOEvaluator(dataset_name, cfg, False, output_folder)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#new for batch size 12\n",
        "#from .detectron2.tools.train_net import Trainer\n",
        "#from detectron2.engine import DefaultTrainer\n",
        "# select from modelzoo here: https://github.com/facebookresearch/detectron2/blob/master/MODEL_ZOO.md#coco-object-detection-baselines\n",
        "\n",
        "from detectron2.config import get_cfg\n",
        "#from detectron2.evaluation.coco_evaluation import COCOEvaluator\n",
        "import os\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\"))\n",
        "cfg.DATASETS.TRAIN = (\"my_dataset_train\",)\n",
        "cfg.DATASETS.TEST = (\"my_dataset_val\",)\n",
        "\n",
        "\n",
        "cfg.DATALOADER.NUM_WORKERS = 4\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\")\n",
        "cfg.SOLVER.IMS_PER_BATCH = 12\n",
        "cfg.SOLVER.BASE_LR = 0.01\n",
        "\n",
        "cfg.SOLVER.WARMUP_ITERS = 50\n",
        "cfg.SOLVER.MAX_ITER = 1492  # Adjusted to 1492 iterations for 100 epochs\n",
        "cfg.SOLVER.STEPS = (746, 1119)  # Adjusted for 50% and 75% of total iterations\n",
        "cfg.SOLVER.GAMMA = 0.005\n",
        "\n",
        "\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 2 #your number of classes + 1\n",
        "\n",
        "cfg.TEST.EVAL_PERIOD = 150\n",
        "\n",
        "\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "trainer = DefaultTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "j_aoJGBCSl4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nA_bw66Hrezs"
      },
      "outputs": [],
      "source": [
        "# Look at training curves in tensorboard:\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvgRMDU3uEi8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# List the contents of the ./output/ directory\n",
        "output_files = os.listdir('./output/')\n",
        "print(output_files)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test evaluation\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "predictor = DefaultPredictor(cfg)\n",
        "evaluator = COCOEvaluator(\"my_dataset_test\", cfg, False, output_dir=\"./output/\")\n",
        "val_loader = build_detection_test_loader(cfg, \"my_dataset_test\")\n",
        "inference_on_dataset(trainer.model, val_loader, evaluator)"
      ],
      "metadata": {
        "id": "XJ6t7dScKzIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from sklearn.metrics import precision_recall_curve, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "\n",
        "cfg.DATASETS.TEST = (\"my_dataset_test\", )\n",
        "\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "test_metadata = MetadataCatalog.get(\"my_dataset_test\")\n",
        "\n",
        "test_loader = build_detection_test_loader(cfg, \"my_dataset_test\")\n",
        "\n",
        "evaluator = COCOEvaluator(\"my_dataset_test\", cfg, False, output_dir=\"./output/\")\n",
        "predictions = inference_on_dataset(predictor.model, test_loader, evaluator)\n",
        "\n",
        "\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "for prediction in predictions:\n",
        "    instances = prediction[\"instances\"]\n",
        "    gt_boxes = instances.gt_boxes.tensor.numpy() if \"gt_boxes\" in instances.fields() else None\n",
        "    scores = instances.scores.numpy()\n",
        "\n",
        "\n",
        "    if gt_boxes is not None:\n",
        "        true_labels.extend([1] * len(gt_boxes))\n",
        "\n",
        "    predicted_labels.extend([1 if score > cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST else 0 for score in scores])\n",
        "\n",
        "true_labels = np.array(true_labels)\n",
        "predicted_labels = np.array(predicted_labels)"
      ],
      "metadata": {
        "id": "SyyAUrBRvHXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at AP, mAP and other evaluation curves curves in tensorboard:\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir output"
      ],
      "metadata": {
        "id": "b7w1BFwGIeMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# List the contents of the ./output/ directory\n",
        "output_files = os.listdir('./output/')\n",
        "print(output_files)"
      ],
      "metadata": {
        "id": "-MmXu7xIJQqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRtsYTaru-OS"
      },
      "outputs": [],
      "source": [
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "cfg.DATASETS.TEST = (\"my_dataset_test\", )\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # set the testing threshold for this model\n",
        "predictor = DefaultPredictor(cfg)\n",
        "test_metadata = MetadataCatalog.get(\"my_dataset_test\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "cfg.DATASETS.TEST = (\"my_dataset_test\", )\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # set the testing threshold for this model\n",
        "predictor = DefaultPredictor(cfg)\n",
        "test_metadata = MetadataCatalog.get(\"my_dataset_test\")\n",
        "\n",
        "from detectron2.utils.visualizer import ColorMode\n",
        "import glob\n",
        "\n",
        "for imageName in glob.glob('/content/palletised_box-1/test/*jpg'):\n",
        "  im = cv2.imread(imageName)\n",
        "  outputs = predictor(im)\n",
        "  v = Visualizer(im[:, :, ::-1],\n",
        "                metadata=test_metadata,\n",
        "                scale=0.8\n",
        "                 )\n",
        "  out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "  cv2_imshow(out.get_image()[:, :, ::-1])\n"
      ],
      "metadata": {
        "id": "toBk2vFRupLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "cfg.DATASETS.TEST = (\"my_dataset_test\", )\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.25   # set the testing threshold for this model\n",
        "predictor = DefaultPredictor(cfg)\n",
        "test_metadata = MetadataCatalog.get(\"my_dataset_test\")\n",
        "\n",
        "from detectron2.utils.visualizer import ColorMode\n",
        "import glob\n",
        "\n",
        "for imageName in glob.glob('/content/palletised_box-1/test/*jpg'):\n",
        "  im = cv2.imread(imageName)\n",
        "  outputs = predictor(im)\n",
        "  v = Visualizer(im[:, :, ::-1],\n",
        "                metadata=test_metadata,\n",
        "                scale=0.8\n",
        "                 )\n",
        "  out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "  cv2_imshow(out.get_image()[:, :, ::-1])\n"
      ],
      "metadata": {
        "id": "EVVrKYU_88W0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "cfg.DATASETS.TEST = (\"my_dataset_test\", )\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.75   # set the testing threshold for this model\n",
        "predictor = DefaultPredictor(cfg)\n",
        "test_metadata = MetadataCatalog.get(\"my_dataset_test\")\n",
        "\n",
        "from detectron2.utils.visualizer import ColorMode\n",
        "import glob\n",
        "\n",
        "for imageName in glob.glob('/content/palletised_box-1/test/*jpg'):\n",
        "  im = cv2.imread(imageName)\n",
        "  outputs = predictor(im)\n",
        "  v = Visualizer(im[:, :, ::-1],\n",
        "                metadata=test_metadata,\n",
        "                scale=0.8\n",
        "                 )\n",
        "  out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "  cv2_imshow(out.get_image()[:, :, ::-1])\n"
      ],
      "metadata": {
        "id": "q8Bha8BE9FYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "cfg.DATASETS.TEST = (\"my_dataset_test\", )\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.50   # set the testing threshold for this model\n",
        "predictor = DefaultPredictor(cfg)\n",
        "test_metadata = MetadataCatalog.get(\"my_dataset_test\")\n",
        "\n",
        "from detectron2.utils.visualizer import ColorMode\n",
        "import glob\n",
        "\n",
        "for imageName in glob.glob('/content/palletised_box-1/test/*jpg'):\n",
        "  im = cv2.imread(imageName)\n",
        "  outputs = predictor(im)\n",
        "  v = Visualizer(im[:, :, ::-1],\n",
        "                metadata=test_metadata,\n",
        "                scale=0.8\n",
        "                 )\n",
        "  out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "  cv2_imshow(out.get_image()[:, :, ::-1])\n"
      ],
      "metadata": {
        "id": "UPgUqjVv-Elk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import glob\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "for imageName in glob.glob('/content/palletised_box-1/test/*.jpg'):\n",
        "\n",
        "    im = cv2.imread(imageName)\n",
        "\n",
        "\n",
        "    outputs = predictor(im)\n",
        "\n",
        "\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "\n",
        "\n",
        "    for i in range(len(instances)):\n",
        "\n",
        "        bbox = instances.pred_boxes.tensor.numpy()[i]\n",
        "\n",
        "\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        width = int(x2 - x1)\n",
        "        length = int(y2 - y1)\n",
        "\n",
        "\n",
        "        cv2.rectangle(im, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
        "\n",
        "\n",
        "        print(f\"Image: {imageName}, Instance {i + 1}: W = {width}mm, L = {length}mm\")\n",
        "\n",
        "\n",
        "        cv2.putText(im, f\"W: {width}mm, L: {length}mm\", (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "\n",
        "    cv2_imshow(im)\n",
        "\n",
        "\n",
        "    import cv2\n",
        "import glob\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "for imageName in glob.glob('/content/parcel-2/test/*.jpg'):\n",
        "\n",
        "    im = cv2.imread(imageName)\n",
        "\n",
        "\n",
        "    outputs = predictor(im)\n",
        "\n",
        "\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "\n",
        "\n",
        "    for i in range(len(instances)):\n",
        "        # Get the bounding box coordinates of the instance\n",
        "        bbox = instances.pred_boxes.tensor.numpy()[i]\n",
        "\n",
        "\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        width = int(x2 - x1)\n",
        "        length = int(y2 - y1)\n",
        "\n",
        "\n",
        "        if width * length > 300:\n",
        "            box_size = \"Large Box\"\n",
        "        elif width * length > 150:\n",
        "            box_size = \"Medium Box\"\n",
        "        else:\n",
        "            box_size = \"Small Box\"\n",
        "\n",
        "        # Overlay bounding box on the image\n",
        "        cv2.rectangle(im, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
        "\n",
        "        # Print the box size on the image\n",
        "        cv2.putText(im, box_size, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "    # Display the image with bounding boxes and box sizes\n",
        "    cv2_imshow(im)\n",
        "\n"
      ],
      "metadata": {
        "id": "MUpbAgex-R1P"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPvEbSTfF0Ev"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import glob\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "for imageName in glob.glob('/content/palletised_box-1/test/*.jpg'):\n",
        "\n",
        "    im = cv2.imread(imageName)\n",
        "\n",
        "\n",
        "    outputs = predictor(im)\n",
        "\n",
        "\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "\n",
        "\n",
        "    for i in range(len(instances)):\n",
        "        # Get the bounding box coordinates of the instance\n",
        "        bbox = instances.pred_boxes.tensor.numpy()[i]\n",
        "\n",
        "\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        width = int(x2 - x1)\n",
        "        length = int(y2 - y1)\n",
        "\n",
        "\n",
        "        cv2.rectangle(im, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
        "\n",
        "\n",
        "        print(f\"Image: {imageName}, Instance {i + 1}: W = {width}mm, L = {length}mm\")\n",
        "\n",
        "\n",
        "        cv2.putText(im, f\"W: {width}mm, L: {length}mm\", (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "\n",
        "    cv2_imshow(im)\n",
        "\n",
        "\n",
        "    import cv2\n",
        "import glob\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "for imageName in glob.glob('/content/parcel-2/test/*.jpg'):\n",
        "\n",
        "    im = cv2.imread(imageName)\n",
        "\n",
        "\n",
        "    outputs = predictor(im)\n",
        "\n",
        "\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "\n",
        "\n",
        "    for i in range(len(instances)):\n",
        "\n",
        "        bbox = instances.pred_boxes.tensor.numpy()[i]\n",
        "\n",
        "        # Calculating width and height of the bounding box\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        width = int(x2 - x1)\n",
        "        length = int(y2 - y1)\n",
        "\n",
        "        if width * length > 300:\n",
        "            box_size = \"Large Box\"\n",
        "        elif width * length > 150:\n",
        "            box_size = \"Medium Box\"\n",
        "        else:\n",
        "            box_size = \"Small Box\"\n",
        "\n",
        "        cv2.rectangle(im, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
        "\n",
        "\n",
        "        cv2.putText(im, box_size, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "\n",
        "    cv2_imshow(im)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxXl6PqWiDha"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import glob\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "for imageName in glob.glob('/content/palletised_box-1/test/*.jpg'):\n",
        "\n",
        "    im = cv2.imread(imageName)\n",
        "\n",
        "\n",
        "    outputs = predictor(im)\n",
        "\n",
        "\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "\n",
        "\n",
        "    for i in range(len(instances)):\n",
        "        # Get the bounding box coordinates of the instance\n",
        "        bbox = instances.pred_boxes.tensor.numpy()[i]\n",
        "\n",
        "        # Calculating width and height of the bounding box\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        width = int(x2 - x1)\n",
        "        length = int(y2 - y1)\n",
        "\n",
        "        # Calculating the area of the bounding box\n",
        "        area = width * length\n",
        "\n",
        "        # Determining the box size based on the area as an extension of the fyp\n",
        "        if area > 300:\n",
        "            width = \"Large Box\"\n",
        "        elif area > 150:\n",
        "            width = \"Medium Box\"\n",
        "        else:\n",
        "            box_size = \"Small Box\"\n",
        "\n",
        "\n",
        "        cv2.rectangle(im, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
        "\n",
        "\n",
        "        cv2.putText(im, box_size, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "\n",
        "    cv2_imshow(im)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import glob\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "#with pixel to mm scale factor\n",
        "#Assuming a scale factor of 0.5 just to show an example for FYP (1 pixel = 0.5 millimeters)\n",
        "pixel_to_mm_scale_factor = 0.5\n",
        "\n",
        "\n",
        "for imageName in glob.glob('/content/palletised_box-1/test/*.jpg'):\n",
        "\n",
        "    im = cv2.imread(imageName)\n",
        "\n",
        "\n",
        "    outputs = predictor(im)\n",
        "\n",
        "\n",
        "    instances = outputs[\"instances\"].to(\"cpu\")\n",
        "\n",
        "\n",
        "    for i in range(len(instances)):\n",
        "\n",
        "        bbox = instances.pred_boxes.tensor.numpy()[i]\n",
        "\n",
        "\n",
        "        x1, y1, x2, y2 = bbox\n",
        "        width_px = int(x2 - x1)\n",
        "        length_px = int(y2 - y1)\n",
        "\n",
        "        # Convert pixel dimensions to millimeters using the scale factor\n",
        "        width_mm = width_px * pixel_to_mm_scale_factor\n",
        "        length_mm = length_px * pixel_to_mm_scale_factor\n",
        "\n",
        "\n",
        "        cv2.rectangle(im, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
        "\n",
        "\n",
        "        print(f\"Image: {imageName}, Instance {i + 1}: W = {width_mm:.2f}mm, L = {length_mm:.2f}mm\")\n",
        "\n",
        "\n",
        "        cv2.putText(im, f\"W: {width_mm:.2f}mm, L: {length_mm:.2f}mm\", (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "\n",
        "    cv2_imshow(im)\n"
      ],
      "metadata": {
        "id": "4R9MxCd0tzGM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "15ItIw654X0s0OBzSGgOicWa2AgQDahL9",
      "authorship_tag": "ABX9TyM6RTlwD58EHkYFuu8fz7up"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}